'''
Python script (scikit-learn) implementing a random forest classifier for a generated dataset
'''

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
import unveil_tree_structure

'''
A randomly generated n-class classification problem. Here there are n_features features, of which n_informative
are "important" for classification into the n_classes classes. The "redundant" features are linear combinations
of "informative" features: they give no new information. Remaining features are "useless" (low importance).
The data is generated by spawning a given number (n_clusters_per_class) of clusters of points per class, each with a normal distribution about vertices of a hypercube in the feature space
'''
X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_redundant=0, n_classes=3, \
             n_clusters_per_class=2, random_state=0, shuffle=False)

'''
The random forest classifier implements an ensemble of n_estimators decision trees to solve the classification 
problem. To train each tree in the forest, a random subset of the training data set is used (this is "bootstrap
aggregation", or "bagging"). Note that with the optional argument "oob_score", out-of-bag (oob) instances not
used for a particular predictor (here, tree) can be used as test data to estimate the accuracy of the predictor.
Regularisation is controlled by the hyperparameters determining the form of the decision trees (e.g.
max_leaf_nodes, max_depth). 
The (default) Gini impurity criterion is used in the CART algorithm to train decision trees (i.e. is used to
determine the feature k and threshold t_k on which each binary decision is based). The Gini impurity essentially
measures the extent of misclassification, and we aim to minimise the cost function based on the Gini impurity
when choosing a split in a tree. Alternatively, we could attempt to minimise a cost function based on the
(information gain) entropy: the entropy of a set is zero when it contains instances of only a single class.
Note that by default not all features are considered when looking for the best split at a given node. Instead,
only a random subset of features are considered. This results in a greater tree diversity and reduces bias.
This choice is controlled by the "max_features" keyword.
'''
rf_clf = RandomForestClassifier(n_estimators=10, max_leaf_nodes=8, bootstrap=True, random_state=0)
rf_clf.fit(X,y)

print "Feature importances:\n", rf_clf.feature_importances_
print "The random forest has determined that there are %i classes:" % len(rf_clf.classes_)
print rf_clf.classes_
leaf_idcs = rf_clf.apply(X) # leaf class indices (of each tree in the forest) for each data point in X
unveil_tree_structure.get_output_info(rf_clf.predict(X),y)
